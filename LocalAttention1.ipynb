{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Xg9-JCFsACg",
        "outputId": "5b1cdea7-eaad-48ed-8fac-c0829b82675f"
      },
      "outputs": [],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK3m8zgOsH3w",
        "outputId": "c36f6cef-e725-4b7f-cd30-cc925080913a"
      },
      "outputs": [],
      "source": [
        "#downloading the data\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SECgr_BsX8s",
        "outputId": "035a2bf8-fe10-4e2e-e7a4-6f7fb0f78bfc"
      },
      "outputs": [],
      "source": [
        "!unzip Flickr8k_Dataset.zip -d all_images\n",
        "!unzip Flickr8k_text.zip -d all_captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ywV55-OslZH",
        "outputId": "e21856e2-c4a2-4d68-f632-17e27bbe59b3"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud\n",
        "!pip install gtts\n",
        "!pip install playsound\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "import string\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import Input\n",
        "from PIL import Image\n",
        "#used for creating Progress Meters or Progress Bars\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHqjAf_ssvTH",
        "outputId": "f3ae2631-26d8-4e88-9ffc-a301b26183a3"
      },
      "outputs": [],
      "source": [
        "!pip install pygobject\n",
        "import glob\n",
        "from gtts import gTTS\n",
        "from playsound import playsound\n",
        "from IPython import display\n",
        "import collections\n",
        "import wordcloud\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OojE7LeUs4Ye",
        "outputId": "4a470814-514f-4637-b7d0-07bafccccfb5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xaicDE9tB_9",
        "outputId": "47c77e96-2168-4abb-a371-648e67a20776"
      },
      "outputs": [],
      "source": [
        "images='/content/all_images/Flicker8k_Dataset'\n",
        "all_imgs = glob.glob(images + '/*.jpg',recursive=True)\n",
        "print(\"The total images present in the dataset: {}\".format(len(all_imgs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "2yxw7qCntLv-",
        "outputId": "a16e1063-83e8-4221-ecd1-7fb4719944bd"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "#Visualising first 5 images :\n",
        "Display_Images = all_imgs[0:5]\n",
        "figure, axes = plt.subplots(1,5)\n",
        "figure.set_figwidth(20)\n",
        "for ax, image in zip(axes, Display_Images):\n",
        "  ax.imshow(imageio.imread(image), cmap=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "SwB8JykEtcQw",
        "outputId": "c413137d-08c4-4bb3-d457-62af8276358d"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import random\n",
        "Image.open(all_imgs[random.randrange(40, 60, 3)])# view a random image\n",
        "import random\n",
        "Image.open(all_imgs[random.randrange(40, 60, 3)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58sHvbxWtrjf",
        "outputId": "66eb1d4b-41f9-4d00-be99-c63d3173a68f"
      },
      "outputs": [],
      "source": [
        "text_file = '/content/gdrive/MyDrive/Data_set/captions.txt'\n",
        "def load_doc(text_file):\n",
        "    open_file = open(text_file, 'r', encoding='latin-1' )\n",
        "    text = open_file.read()\n",
        "    open_file.close()\n",
        "    return text\n",
        "doc = load_doc(text_file)\n",
        "print(doc[:300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "qrs9q_SLt0Ql",
        "outputId": "3b93d174-8aed-44a7-9e8f-ee27f5209816"
      },
      "outputs": [],
      "source": [
        "img_path = '/content/gdrive/MyDrive/Data_set/Images/'\n",
        "\n",
        "all_img_id = []\n",
        "all_img_vector = []\n",
        "annotations = []\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Data_set/captions.txt' , 'r') as fo:\n",
        "  next(fo)\n",
        "  for line in fo :\n",
        "    split_arr = line.split(',')\n",
        "    all_img_id.append(split_arr[0])\n",
        "    annotations.append(split_arr[1].rstrip('\\n.')) #removing out the \\n.\n",
        "    all_img_vector.append(img_path+split_arr[0])\n",
        "\n",
        "df = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions'])\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah0_y-J_t9Eu",
        "outputId": "be2d7be3-0264-4bfd-8372-87ccce895de8"
      },
      "outputs": [],
      "source": [
        "print(\"Total captions present in the dataset: \"+ str(len(annotations)))\n",
        "print(\"Total images present in the dataset: \" + str(len(all_imgs)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBoe2MmZuDG2",
        "outputId": "c39d3be7-901a-48f4-8862-ada82809e194"
      },
      "outputs": [],
      "source": [
        "#Create the vocabulary & the counter for the captions\n",
        "vocabulary = [word.lower() for line in annotations for word in line.split()]\n",
        "val_count = Counter(vocabulary)\n",
        "val_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cYM2dHrvuLPm",
        "outputId": "caf40840-89eb-4435-ffb7-407f27b9c070"
      },
      "outputs": [],
      "source": [
        "#Visualise the top 30 occuring words in the captions\n",
        "for word, count in val_count.most_common(30):\n",
        "  print(word, \": \", count)\n",
        "\n",
        "lst = val_count.most_common(30)\n",
        "most_common_words_df = pd.DataFrame(lst, columns = ['Word', 'Count'])\n",
        "most_common_words_df.plot.bar(x='Word', y='Count', width=0.6, color='orange', figsize=(15, 10))\n",
        "plt.title(\"Top 30 maximum frequency words\", fontsize = 18, color= 'navy')\n",
        "plt.xlabel(\"Words\", fontsize = 14, color= 'navy')\n",
        "plt.ylabel(\"Count\", fontsize = 14, color= 'navy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "B10PsN_5uWIR",
        "outputId": "5e8f478f-fe2b-4d8e-e027-6caa02e21265"
      },
      "outputs": [],
      "source": [
        "wordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(val_count)\n",
        "plt.figure(figsize = (12, 12))\n",
        "plt.imshow(wordcloud)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "kPnNRHeYudnE",
        "outputId": "f86a8125-0587-4069-9cfe-53e92740bbea"
      },
      "outputs": [],
      "source": [
        "def caption_with_img_plot(image_id, frame) :\n",
        "  capt = (\"\\n\" *2).join(frame[frame['ID'] == image_id].Captions.to_list())\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.set_axis_off()\n",
        "  idx = df.ID.to_list().index(image_id)\n",
        "  im =  Image.open(df.Path.iloc[idx])\n",
        "  w, h = im.size[0], im.size[-1]\n",
        "  ax.imshow(im)\n",
        "  ax.text(w+50, h, capt, fontsize = 18, color = 'navy')\n",
        "caption_with_img_plot(df.ID.iloc[8049], df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2aGBqWdukdy"
      },
      "outputs": [],
      "source": [
        "#data cleaning\n",
        "rem_punct = str.maketrans('', '', string.punctuation)\n",
        "for r in range(len(annotations)) :\n",
        "  line = annotations[r]\n",
        "  line = line.split()\n",
        "\n",
        "  # converting to lowercase\n",
        "  line = [word.lower() for word in line]\n",
        "\n",
        "  # remove punctuation from each caption and hanging letters\n",
        "  line = [word.translate(rem_punct) for word in line]\n",
        "  line = [word for word in line if len(word) > 1]\n",
        "\n",
        "  # remove numeric values\n",
        "  line = [word for word in line if word.isalpha()]\n",
        "\n",
        "  annotations[r] = ' '.join(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bP7XP-rurgZ"
      },
      "outputs": [],
      "source": [
        "#add the <start> & <end> token to all those captions as well\n",
        "annotations = ['<start>' + ' ' + line + ' ' + '<end>' for line in annotations]\n",
        "\n",
        "#Create a list which contains all the path to the images\n",
        "all_img_path = all_img_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krpJ7iQSuwLh",
        "outputId": "3e74cea7-00e8-4202-9b94-f5b6b458b14b"
      },
      "outputs": [],
      "source": [
        "##list contatining captions for an image\n",
        "annotations[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw5XYJB8u3Me"
      },
      "outputs": [],
      "source": [
        "# Creating the tokenizer\n",
        "top_word_cnt = 5000\n",
        "tokenizer = Tokenizer(num_words = top_word_cnt+1, filters= '!\"#$%^&*()_+.,:;-?/~`{}[]|\\=@ ',\n",
        "                      lower = True, char_level = False,\n",
        "                      oov_token = 'UNK')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZMnalxTu9FN"
      },
      "outputs": [],
      "source": [
        "# Creating word-to-index and index-to-word mappings.\n",
        "\n",
        "tokenizer.fit_on_texts(annotations)\n",
        "#transform each text into a sequence of integers\n",
        "train_seqs = tokenizer.texts_to_sequences(annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tp8zT32_vBJx"
      },
      "outputs": [],
      "source": [
        "# We add PAD token for zero\n",
        "tokenizer.word_index['PAD'] = 0\n",
        "tokenizer.index_word[0] = 'PAD'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxjQLvkBvFcN",
        "outputId": "ab89964c-21ef-4668-cdd5-aafbcb071415"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.oov_token)\n",
        "print(tokenizer.index_word[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vnrcB8tvPgS",
        "outputId": "513429db-9441-4857-90d6-e6b9aaf5cfb8"
      },
      "outputs": [],
      "source": [
        "tokenizer.index_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX7XZxlBvQ0Y"
      },
      "outputs": [],
      "source": [
        "# Creating a word count for our tokenizer to visualize the Top 30 occuring words after text processing\n",
        "\n",
        "tokenizer_top_words = [word for line in annotations for word in line.split() ]\n",
        "\n",
        "#tokenizer_top_words_count\n",
        "tokenizer_top_words_count = collections.Counter(tokenizer_top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mqtUnSvIvtMJ",
        "outputId": "2e521ba6-e384-4030-a67e-8420cb02e1e8"
      },
      "outputs": [],
      "source": [
        "for word, count in tokenizer_top_words_count.most_common(30) :\n",
        "  print(word, \": \", count)\n",
        "\n",
        "tokens = tokenizer_top_words_count.most_common(30)\n",
        "most_com_words_df = pd.DataFrame(tokens, columns = ['Word', 'Count'])\n",
        "\n",
        "#plot 30 most common words\n",
        "most_common_words_df.plot.bar(x = 'Word', y= 'Count', width=0.8, color = 'indigo', figsize = (17, 10))\n",
        "plt.title('Top 30 common words', fontsize =20, color= 'navy')\n",
        "plt.xlabel('Words', fontsize =14, color= 'navy')\n",
        "plt.ylabel('Counts', fontsize =14, color= 'navy')\n",
        "#plt.grid_b(b=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "mud1OK7ovxXN",
        "outputId": "49ba2c48-10c4-4ea2-e006-0480f02e1558"
      },
      "outputs": [],
      "source": [
        "wordcloud_token = WordCloud(width = 1000, height = 500).generate_from_frequencies(tokenizer_top_words_count)\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.imshow(wordcloud_token)\n",
        "#plt.grid(b = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptvU4ZWyv49d",
        "outputId": "31877280-e811-4c7a-c67a-e6baa86ba98e"
      },
      "outputs": [],
      "source": [
        "# Pad each vector to the max_length of the captions  store it to a vairable\n",
        "\n",
        "train_seqs_len = [len(seq) for seq in train_seqs]\n",
        "\n",
        "longest_word_length = max(train_seqs_len)\n",
        "\n",
        "cap_vector= tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding= 'post', maxlen = longest_word_length,\n",
        "                                                          dtype='int32', value=0)\n",
        "print(\"The shape of Caption vector is :\" + str(cap_vector.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eKZ56cJwalF",
        "outputId": "0ae25a1d-3c15-4354-9312-49ffdd4a7726"
      },
      "outputs": [],
      "source": [
        "# Pad each vector to the max_length of the captions  store it to a vairable\n",
        "\n",
        "train_seqs_len = [len(seq) for seq in train_seqs]\n",
        "\n",
        "longest_word_length = max(train_seqs_len)\n",
        "\n",
        "cap_vector= tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding= 'post', maxlen = longest_word_length,\n",
        "                                                          dtype='int32', value=0)\n",
        "print(\"The shape of Caption vector is :\" + str(cap_vector.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01xmMLDewg1w"
      },
      "outputs": [],
      "source": [
        "# creating list to store preprocessed images and setting up the Image Shape\n",
        "\n",
        "preprocessed_image = []\n",
        "IMAGE_SHAPE = (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rOmmEBqGwoop",
        "outputId": "ac305df6-ffde-46a6-a985-b14011f36910"
      },
      "outputs": [],
      "source": [
        "#checking image format\n",
        "import tensorflow as tf\n",
        "tf.keras.backend.image_data_format()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTtyVD2Awz72"
      },
      "outputs": [],
      "source": [
        "for img in all_imgs[0:5] :\n",
        "    img = tf.io.read_file(img, name=None)\n",
        "    img = tf.image.decode_jpeg(img, channels=0)\n",
        "    img = tf.image.resize(img, (224, 224))\n",
        "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
        "    preprocessed_image.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "dJwlS8VVxTNf",
        "outputId": "6904068d-95d7-4eaa-8a00-4868e08adefb"
      },
      "outputs": [],
      "source": [
        "# checking first five images post preprocessing\n",
        "Display_Images = preprocessed_image[0:5]\n",
        "figure, axes = plt.subplots(1,5)\n",
        "figure.set_figwidth(25)\n",
        "for ax, image in zip(axes, Display_Images) :\n",
        "  print('Shape after resize : ', image.shape)\n",
        "  ax.imshow(image)\n",
        "  ax.grid('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unqSbxJWxgv_"
      },
      "outputs": [],
      "source": [
        "def load_images(image_path) :\n",
        "  img = tf.io.read_file(image_path, name = None)\n",
        "  img = tf.image.decode_jpeg(img, channels=0)\n",
        "  img = tf.image.resize(img, IMAGE_SHAPE)\n",
        "  img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "  return img, image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwdsw6msxmI6",
        "outputId": "b4fe718a-d423-489b-e0d8-2f23c3c5b6ee"
      },
      "outputs": [],
      "source": [
        "all_img_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE5S5G0yxs4j"
      },
      "outputs": [],
      "source": [
        "training_list = sorted(set(all_img_vector))\n",
        "New_Img = tf.data.Dataset.from_tensor_slices(training_list)\n",
        "New_Img = New_Img.map(load_images, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "New_Img = New_Img.batch(64, drop_remainder=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfeZB9UXxt1P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy32TaL1xxbr"
      },
      "outputs": [],
      "source": [
        "#Ratio = 80:20 and we will set random state = 42\n",
        "path_train, path_test, caption_train, caption_test = train_test_split(all_img_vector, cap_vector, test_size = 0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y56A3_eUx3KP",
        "outputId": "3de2b135-4f15-43b4-c2ea-2bfcf669df20"
      },
      "outputs": [],
      "source": [
        "print(\"Training data for images: \" + str(len(path_train)))\n",
        "print(\"Testing data for images: \" + str(len(path_test)))\n",
        "print(\"Training data for Captions: \" + str(len(caption_train)))\n",
        "print(\"Testing data for Captions: \" + str(len(caption_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf5ZdMhnx-ux",
        "outputId": "3ce2c764-680a-4a53-90fd-12cc18a7928b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "image_model = tf.keras.applications.ResNet50(input_shape=(224,224,3), include_top=False, weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THik_A6pyE7i",
        "outputId": "d2e069ac-49c2-4f20-e6f5-9d73f57e7602"
      },
      "outputs": [],
      "source": [
        "# Once the features are created, you need to reshape them such that feature shape is in order of (batch_size, 8*8, 2048)\n",
        "image_features_extract_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPK8dLpHyKw1",
        "outputId": "f31b1525-6623-489b-8647-bfad9ed3c46a"
      },
      "outputs": [],
      "source": [
        "# extract features from each image in the dataset\n",
        "img_features = {}\n",
        "for image, image_path in tqdm(New_Img) :\n",
        "  batch_features = image_features_extract_model(image)\n",
        "  #squeeze out the features in a batch\n",
        "  batch_features_flattened = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "  for batch_feat, path in zip(batch_features_flattened, image_path) :\n",
        "    feature_path = path.numpy().decode('utf-8')\n",
        "    img_features[feature_path] = batch_feat.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZflfnSLyRRr",
        "outputId": "a622cb65-b4fd-4170-f83c-24838e90e616"
      },
      "outputs": [],
      "source": [
        "batch_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABeMINO8yWw6",
        "outputId": "6bf13203-4310-4d51-e6b6-e6ae156bad72"
      },
      "outputs": [],
      "source": [
        "batch_features_flattened"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cafKBzrpya6u",
        "outputId": "dd60591c-cd9f-49ad-ccf7-5ab5a0f6eb02"
      },
      "outputs": [],
      "source": [
        "batch_feat.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzLPsJkYyeZm",
        "outputId": "56fb7af8-6eee-4370-b2b6-63f43ffee6a8"
      },
      "outputs": [],
      "source": [
        "#view top five items of img_features dict\n",
        "import more_itertools\n",
        "top_5 = more_itertools.take(5, img_features.items())\n",
        "top_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZuqlQDFyluO"
      },
      "outputs": [],
      "source": [
        "#to provide, both images along with the captions as input\n",
        "def map(image_name, caption):\n",
        "    img_tensor = img_features[image_name.decode('utf-8')]\n",
        "    return img_tensor, caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibpZWCcUymwm"
      },
      "outputs": [],
      "source": [
        "# This function should transform the created dataset(img_path,cap) to (features,cap) using the map_func created earlier\n",
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 32\n",
        "def gen_dataset(img, capt):\n",
        "\n",
        "    data = tf.data.Dataset.from_tensor_slices((img, capt))\n",
        "    data = data.map(lambda ele1, ele2 : tf.numpy_function(map, [ele1, ele2], [tf.float32, tf.int32]),\n",
        "                    num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "    data = (data.shuffle(BUFFER_SIZE, reshuffle_each_iteration= True).batch(BATCH_SIZE, drop_remainder = False)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SbANc18y1QP"
      },
      "outputs": [],
      "source": [
        "train_dataset = gen_dataset(path_train,caption_train)\n",
        "test_dataset = gen_dataset(path_test,caption_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsVkbeCzy4mw",
        "outputId": "c348f507-c732-4b42-ac33-10b9a78fdbe4"
      },
      "outputs": [],
      "source": [
        "sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n",
        "print(sample_img_batch.shape)  #(batch_size, 8*8, 2048)\n",
        "print(sample_cap_batch.shape) #(batch_size,max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rvt5Xmdy5zk"
      },
      "outputs": [],
      "source": [
        " #Setting  parameters\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "\n",
        "#top 5,000 words +1\n",
        "vocab_size = 5001\n",
        "train_num_steps = len(path_train) // BATCH_SIZE\n",
        "test_num_steps = len(path_test) // BATCH_SIZE\n",
        "\n",
        "max_length = 31\n",
        "feature_shape = batch_feat.shape[1]\n",
        "attention_feature_shape = batch_feat.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zETTv40My_Ws"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOwoWFeCzCPy",
        "outputId": "ab71ad29-5faf-4982-e718-1505b1401979"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "print(tf.compat.v1.get_default_graph())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw0BK4_FzOLd"
      },
      "outputs": [],
      "source": [
        "#Building Encoder using CNN Keras subclassing method\n",
        "\n",
        "class Encoder(Model):\n",
        "    def __init__(self,embed_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dense = tf.keras.layers.Dense(embed_dim) #build your Dense layer with relu activation\n",
        "\n",
        "    def call(self, features):\n",
        "        features =  self.dense(features) # extract the features from the image shape: (batch, 8*8, embed_dim)\n",
        "        features =  tf.keras.activations.relu(features, alpha=0.01, max_value=None, threshold=0)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQhFc9LZzoCj"
      },
      "outputs": [],
      "source": [
        "encoder=Encoder(embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwaPrPCwzsLE"
      },
      "outputs": [],
      "source": [
        "#from tensorflow.keras.utils import plot_model\n",
        "#import pydot\n",
        "from keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1hjIO-IzxiG"
      },
      "outputs": [],
      "source": [
        "class Attention_model(Model):\n",
        "    def __init__(self, units):\n",
        "        super(Attention_model, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "        self.units=units\n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        hidden_with_time_axis = hidden[:, tf.newaxis]\n",
        "        score = tf.keras.activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "        attention_weights = tf.keras.activations.softmax(self.V(score), axis=1)\n",
        "        context_vector = attention_weights * features\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FazB0SXyz7Bu"
      },
      "outputs": [],
      "source": [
        "class Decoder(Model):\n",
        "    def __init__(self, embed_dim, units, vocab_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.units=units\n",
        "        self.attention = Attention_model(self.units) #iniitalise your Attention model with units\n",
        "        self.embed = tf.keras.layers.Embedding(vocab_size, embed_dim) #build your Embedding layer\n",
        "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
        "        self.d1 = tf.keras.layers.Dense(self.units) #build your Dense layer\n",
        "        self.d2 = tf.keras.layers.Dense(vocab_size) #build your Dense layer\n",
        "\n",
        "\n",
        "    def call(self,x,features, hidden):\n",
        "        context_vector, attention_weights = self.attention(features, hidden) #create your context vector & attention weights from attention model\n",
        "        embed = self.embed(x) # embed your input to shape: (batch_size, 1, embedding_dim)\n",
        "        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis = -1) # Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n",
        "        output,state = self.gru(embed) # Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n",
        "        output = self.d1(output)\n",
        "        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)\n",
        "        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)\n",
        "\n",
        "        return output, state, attention_weights\n",
        "\n",
        "    def init_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5S3NJtG0Cp-"
      },
      "outputs": [],
      "source": [
        "decoder=Decoder(embedding_dim, units, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiFLeRP20Fv4",
        "outputId": "3a676b78-43dd-49db-a613-f51e18f5a27a"
      },
      "outputs": [],
      "source": [
        "features=encoder(sample_img_batch)\n",
        "\n",
        "hidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\n",
        "dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n",
        "\n",
        "predictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\n",
        "print('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\n",
        "print('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\n",
        "print('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "UWctilt_0LmH",
        "outputId": "7104774d-b52b-4b04-a978-cd3c3dfd8245"
      },
      "outputs": [],
      "source": [
        "#optimizer = tf.keras.optimizers.Adam(learning_rate = 0.003)  #define the optimizer\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = tf.keras.losses.Reduction.NONE) #define your loss object\n",
        "model.compile(loss_object, optimizer=tf.keras.optimizers.Adam(learning_rate=0.003),metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SHgiIJu0PqK"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    #loss is getting multiplied with mask to get an ideal shape\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "1iIBP7000Syo",
        "outputId": "d5c6d757-16e5-4339-d0af-d18eac061c61"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"Flickr8K/checkpoint1\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thhLpHNI0V4P"
      },
      "outputs": [],
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SngkhyiH0ah-"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        encoder_op = encoder(img_tensor)\n",
        "        for r in range(1, target.shape[1]) :\n",
        "          predictions, hidden, _ = decoder(dec_input, encoder_op, hidden)\n",
        "          loss = loss + loss_function(target[:, r], predictions)\n",
        "          dec_input = tf.expand_dims(target[:, r], 1)\n",
        "\n",
        "    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch\n",
        "    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n",
        "    grad = tape.gradient (loss, trainable_vars)\n",
        "    optimizer.apply_gradients(zip(grad, trainable_vars))\n",
        "\n",
        "    return loss, avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "De-s2PEZ0fXi"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = decoder.init_state(batch_size = target.shape[0])\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "    with tf.GradientTape() as tape:\n",
        "      encoder_op = encoder(img_tensor)\n",
        "      for r in range(1, target.shape[1]) :\n",
        "        predictions, hidden, _ = decoder(dec_input, encoder_op, hidden)\n",
        "        loss = loss + loss_function(target[:, r], predictions)\n",
        "        dec_input = tf.expand_dims(target[: , r], 1)\n",
        "    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch\n",
        "    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n",
        "    grad = tape.gradient (loss, trainable_vars)\n",
        "    optimizer.apply_gradients(zip(grad, trainable_vars))\n",
        "    return loss, avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_58mhOb90i76"
      },
      "outputs": [],
      "source": [
        "def test_loss_cal(test_dataset):\n",
        "    total_loss = 0\n",
        "    for (batch, (img_tensor, target)) in enumerate(test_dataset) :\n",
        "      batch_loss, t_loss = test_step(img_tensor, target)\n",
        "      total_loss = total_loss + t_loss\n",
        "      avg_test_loss = total_loss/ test_num_steps\n",
        "\n",
        "    return avg_test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fz77L8F0nV2"
      },
      "outputs": [],
      "source": [
        "loss_plot = []\n",
        "test_loss_plot = []\n",
        "EPOCHS = 30\n",
        "best_test_loss=100\n",
        "for epoch in tqdm(range(0, EPOCHS)):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "        avg_train_loss=total_loss / train_num_steps\n",
        "    loss_plot.append(avg_train_loss)\n",
        "    test_loss = test_loss_cal(test_dataset)\n",
        "    test_loss_plot.append(test_loss)\n",
        "    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    if test_loss < best_test_loss:\n",
        "        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n",
        "        best_test_loss = test_loss\n",
        "        ckpt_manager.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ji9lA7830roD"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(12, 8))\n",
        "plt.plot(loss_plot, color='orange', label = 'training_loss_plot')\n",
        "plt.plot(test_loss_plot, color='green', label = 'test_loss_plot')\n",
        "plt.xlabel('Epochs', fontsize = 15, color = 'red')\n",
        "plt.ylabel('Loss', fontsize = 15, color = 'red')\n",
        "plt.title('Loss Plot', fontsize = 20, color = 'red')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2AU0s4m0wTK"
      },
      "outputs": [],
      "source": [
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_feature_shape))\n",
        "\n",
        "    hidden = decoder.init_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_images(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder (img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result.append (tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot,predictions\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot,predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8LkMF0_01bK"
      },
      "outputs": [],
      "source": [
        "def plot_attention_map (caption, weights, image) :\n",
        "\n",
        "  fig = plt.figure(figsize = (20, 20))\n",
        "  temp_img = np.array(Image.open(image))\n",
        "\n",
        "  cap_len = len(caption)\n",
        "  for cap in range(cap_len) :\n",
        "    weights_img = np.reshape(weights[cap], (8,8))\n",
        "    wweights_img = np.array(Image.fromarray(weights_img).resize((224,224), Image.LANCZOS))\n",
        "\n",
        "    ax = fig.add_subplot(cap_len//2, cap_len//2, cap+1)\n",
        "    ax.set_title(caption[cap], fontsize = 14, color = 'red')\n",
        "\n",
        "    img = ax.imshow(temp_img)\n",
        "\n",
        "    ax.imshow(weights_img, cmap='gist_heat', alpha=0.6, extent=img.get_extent())\n",
        "    ax.axis('off')\n",
        "  plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHTIwBt504Ny"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM7vjwkN08ii"
      },
      "outputs": [],
      "source": [
        "def filt_text(text):\n",
        "    filt=['<start>','<unk>','<end>']\n",
        "    temp= text.split()\n",
        "    [temp.remove(j) for k in filt for j in temp if k==j]\n",
        "    text=' '.join(temp)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft3kivhO1AXD"
      },
      "outputs": [],
      "source": [
        "image_test = path_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3jhKmOs1F3B"
      },
      "outputs": [],
      "source": [
        "def pred_caption_audio(random, autoplay=False, weights=(0.5, 0.5, 0, 0)) :\n",
        "\n",
        "    cap_test_data = caption_test.copy()\n",
        "    rid = np.random.randint(0, random)\n",
        "    test_image = image_test[rid]\n",
        "    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test_data[rid] if i not in [0]])\n",
        "    result, attention_plot, pred_test = evaluate(test_image)\n",
        "    real_caption=filt_text(real_caption)\n",
        "    pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
        "    real_appn = []\n",
        "    real_appn.append(real_caption.split())\n",
        "    reference = real_appn\n",
        "    candidate = pred_caption.split()\n",
        "    score = sentence_bleu(reference, candidate, weights=weights)#set your weights\n",
        "    print(f\"BLEU score: {score*100}\")\n",
        "    print ('Real Caption:', real_caption)\n",
        "    print ('Prediction Caption:', pred_caption)\n",
        "    plot_attention_map(result, attention_plot, test_image)\n",
        "    speech = gTTS('Predicted Caption : ' + pred_caption, lang = 'en', slow = False)\n",
        "    speech.save('voice.mp3')\n",
        "    audio_file = 'voice.mp3'\n",
        "    display.display(display.Audio(audio_file, rate = None, autoplay = autoplay))\n",
        "    return test_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck7_3zdS1P15"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz_cg3vq1dK_"
      },
      "outputs": [],
      "source": [
        "test_image = pred_caption_audio(len(image_test), True, weights = (0.5, 0.25, 0, 0))\n",
        "Image.open(test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88B9vp3s1jeD"
      },
      "outputs": [],
      "source": [
        "test_image = pred_caption_audio(len(image_test), True, weights = (0.5, 0.25, 0, 0))\n",
        "Image.open(test_image)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
